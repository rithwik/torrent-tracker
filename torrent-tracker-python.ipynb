{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are like most people are today, you have searched for torrents to download stuff. And if you are like me, you have used torrentz.com to conduct those searches, because it aggregates results from various sites.\n",
    "\n",
    "In this post we will see how to do those searches programatically from the terminal. This in it itself will reduce effort and time required to search for something, but using scripts allows us to build upon this basic search functionality. Here we will build up to a system with a pre-set list of items to search for and a way to exclude results that we have already seen.\n",
    "\n",
    "## The Address\n",
    "\n",
    "If you go to your browser and use your normal workflow to search for 'Ubuntu 16.04' you will see something like this. \n",
    "\n",
    "<img src=\"/images/ubuntusearch1.png\">\n",
    "\n",
    "This means that if you were to instead type in the whole address as https://torrentz.eu/search?f=Ubuntu%2016.04 you would have got the same results. We now know that if we can somehow construct the search URL from the items we want to search for, and if we can send a request to the website, then we can get back the data we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://torrentz.eu/search?f=ubuntu+16.04\n"
     ]
    }
   ],
   "source": [
    "base_url = 'https://torrentz.eu/search?f='\n",
    "search_term = 'ubuntu 16.04'\n",
    "# Forget about the '%20's for spaces. Append search term to URL with '+' in between.\n",
    "url = base_url + '+'.join(map(str, search_term.split(' ')))\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Request\n",
    "\n",
    "Sending requests over the internet is a common need and Python had the 'requests' module to help us out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "response = requests.get(url)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we got a Response object with status 200. This means that there were no problems and we have a webpage. If you got other codes like the familiar 404 you would know that your request failed. You can also check if the response was ok by checking the value of the 'ok' attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The page contents itself is stored in the 'text' attribute. Let's see how it looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23455"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'=\"/secureopensearch.xml\" type=\"application/opensearchdescription+xml\" title=\"Torrentz Search\" />\\n<meta name=\"viewport\" content=\"width=820\">\\n</head>\\n<body>\\n<div class=\"top\">\\n<h1><a href=\"/\" title=\"Search Engine\">Torrentz</a></h1>\\n<ul>\\n<li><a href=\"/search\" title=\"Torrentz Search\">Search</a></li>\\n<li><a href=\"/my\" title=\"Personal Search\">myTorrentz</a></li>\\n<li><a href=\"/profile\" title=\"My Profile\">Profile</a></li>\\n<li><a href=\"/help\" title=\"Get Help\">Help</a></li>\\n</ul>\\n</div>\\n\\n<form action=\"/sea'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text[500:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we got a lot of HTML which is not what we needed. We need some way to extract the useful names of the torrents from this noise.\n",
    "\n",
    "## The Extraction\n",
    "\n",
    "If you print out the whole response.text and keep staring at it for some time you will eventually figure out that the results you want are enclosed in the < dt > tags which themseleves are enclosed in < dl > tags.\n",
    "\n",
    "But there is an easier way to find that if you are using Chrome. Right click and choose 'Inspect' or just press Ctrl+Shift+I to reveal a pane with the HTML of the current page laid out in a much more organized fashion. The best part is when you move your mouse over the HTML tags the corresponding area in the page gets highlighted allowing you to quickly find out in which tags the data you are interested in is boxed up.\n",
    "\n",
    "<img src=\"/images/ubuntusearch2.png\">\n",
    "\n",
    "So all we need now is a way to get out only the text which are in < dt > tags inside < dl > tags.\n",
    "\n",
    "Without getting into a discussion about regular expressions, or HTML tag trees, let's use the BeautifulSoup module which was built just for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw element: \n",
      " <dt><a href=\"//ads.ad-center.com/offer?prod=7&amp;ref=5052214\" rel=\"nofollow\"><strong>ubuntu 16.04</strong> [Verified]</a></dt>\n",
      "\n",
      "The relevant text: \n",
      " ubuntu 16.04 [Verified]\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "soup = bs4.BeautifulSoup(response.text, 'lxml')\n",
    "elements = soup.select('dl > dt') # note how we specify that we want <dt>s inside <dl>s\n",
    "print('The raw element: \\n', elements[1])\n",
    "print('\\nThe relevant text: \\n', elements[1].getText())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we are done!\n",
    "\n",
    "Let's put this all together into a function, and while we are at it let's also do a few more useful things:\n",
    "- Keep the number of results down to Top N (using 5 here) because we don't really want all the results\n",
    "- Add one more base url with 'searchA?f' instead of 'search?f' to get the date ordered results too\n",
    "- Make a list of items to search automatically every time the script is run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "Results for UBUNTU 16.04\n",
      "-----------------------------------------\n",
      "Full ubuntu 16.04 Download\n",
      "ubuntu 16.04 [Verified]\n",
      "Direct ubuntu 16.04 Download\n",
      "ubuntu 16 04 desktop 64 bit lts iso\n",
      "Ubuntu 16 04 Desktop x86 iso\n",
      "Full ubuntu 16.04 Download\n",
      "ubuntu 16.04 [Verified]\n",
      "Direct ubuntu 16.04 Download\n",
      "Ubuntu Studio 16 04 DVD i386 ISO DISTRO Netchup\n",
      "Ubuntu Studio 16 04 DVD x86 64 ISO DISTRO Netchup\n",
      "\n",
      "-----------------------------------------\n",
      "Results for ELEMENTARY OS\n",
      "-----------------------------------------\n",
      "Full elementary os Download\n",
      "elementary os [Verified]\n",
      "Direct elementary os Download\n",
      "Windows 7 Elementary 2016 by axeswy & Tomecar =TEAM OS=\n",
      "Elementary OS Freya 64 Bit\n",
      "Full elementary os Download\n",
      "elementary os [Verified]\n",
      "Direct elementary os Download\n",
      "Windows 7 Elementary 2016 by axeswy & Tomecar =TEAM OS=\n",
      "Elementary OS 0 3 1 64 bit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BASE_URLS = ['https://torrentz.eu/search?f=',\n",
    "             'https://torrentz.eu/searchA?f=']\n",
    "\n",
    "NUMBER = 5\n",
    "\n",
    "SEARCH_LIST = ['ubuntu 16.04',\n",
    "               'elementary os']\n",
    "\n",
    "\n",
    "import requests, bs4\n",
    "\n",
    "def search(name):\n",
    "    results = []\n",
    "    for base_url in BASE_URLS:\n",
    "        url = base_url + '+'.join(map(str, name.split(' ')))\n",
    "        response = requests.get(url)\n",
    "        soup = bs4.BeautifulSoup(response.text, 'lxml')\n",
    "        elems = soup.select('dl > dt')\n",
    "        elems = [x.getText().split(' Â» ')[0] for x in elems[:NUMBER]]\n",
    "        for elem in elems:\n",
    "            results.append(elem)\n",
    "    return results\n",
    "            \n",
    "for name in SEARCH_LIST:\n",
    "    results = search(name)\n",
    "    print(\"-----------------------------------------\")\n",
    "    print('Results for ' + name.upper())\n",
    "    print(\"-----------------------------------------\")\n",
    "    print('\\n'.join(results) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you save the code in the above cell in a Python file you can run it in your terminal with a simple \"python file.py\" to see the results. Alternatively, if you are using Linux you can put \"#! /path/to/python\" on the first line, make the script executable by running \"chmod +x file.py\" in the terminal, and then run it more simply by running \"./file.py\"\n",
    "\n",
    "Now there are a couple of features that we would want from this script to really make it useful. We don't really want to go into the file and change the SEARCH_LIST whenever we want to search for a new thing. So we could take the search term from the terminal instead. Also, every time we repeat a search we really want to see only the results which weren't shown the last time. To solve this all we need to do is to write out the results into a text file and each time remove the results which are present in the file.\n",
    "\n",
    "You can find the complete script with these added functionalities and the Jupyter notebook for this post on [Github](https://github.com/rithwik/torrent-tracker)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "nikola": {
   "category": "",
   "date": "2016-06-02 16:47:33 UTC+05:30",
   "description": "",
   "link": "",
   "slug": "torrent-tracker-python",
   "tags": "python, scraping",
   "title": "Tracking Torrents (with Python)",
   "type": "text"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
